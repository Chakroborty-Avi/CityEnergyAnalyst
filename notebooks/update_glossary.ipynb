{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Update glossary.csv file using data from schemas.yml\n\nThis script assumes you have a \"new\" schema obtained from running the `cea trace-inputlocator` script."
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": "import cea.scripts\nimport cea.inputlocator\nimport cea.config\nimport cea.glossary\nfrom cea.tests.trace_inputlocator import get_csv_schema\nimport os\nimport yaml\nimport json"
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": "schemas = cea.scripts.schemas()\nglossary_df = cea.glossary.read_glossary_df()\nlocators = schemas.keys()\n\ndef save_glossary(glossary_df):\n    glossary_df.to_csv(os.path.join(os.path.dirname(cea.glossary.__file__), 'glossary.csv'),\n                  columns=[\"SCRIPT\", \"LOCATOR_METHOD\", \"FILE_NAME\", \"VARIABLE\", \"DESCRIPTION\", \"UNIT\", \"VALUES\", \"TYPE\", \"COLOR\"],\n                  index=False)\n    print(\"saved new glossary.csv - reloading\")\n    glossary_df = cea.glossary.read_glossary_df()\n    return glossary_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### start by finding all entries in schemas.yml without a schema"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "the following three locator methods need \"special\" treatment:\n- get_optimization_checkpoint\n  - \"special\" schema\n- get_optimization_disconnected_cooling_capacity\n  - only present in projects with cooling network\n- get_optimization_connected_cooling_capacity\n  - only present in projects with cooling network\n  \nthis code assumes you have a \"reference-case-cooling/baseline\" in your projectroot and have run the optimization on that (e.g. run `cea workflow --workflow district-cooling-system`)"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": "config = cea.config.Configuration()\nconfig.scenario = os.path.join(config.project, \"..\", \"reference-case-cooling\", \"baseline\")\nlocator = cea.inputlocator.InputLocator(scenario=config.scenario)\n\n# load get_optimization_checkpoint schema\nif not schemas[\"get_optimization_checkpoint\"][\"schema\"]:\n    with open(locator.get_optimization_checkpoint(1), 'r') as fp:\n        get_optimization_checkpoint = json.load(fp)\n    schemas[\"get_optimization_checkpoint\"][\"schema\"] = {\n        str(key): {\"sample_data\": get_optimization_checkpoint[key],\n                   \"types_found\": None}\n        for key in get_optimization_checkpoint.keys()\n    }\n\n# load get_optimization_disconnected_cooling_capacity schema\nif not schemas[\"get_optimization_disconnected_cooling_capacity\"][\"schema\"]:\n    schemas[\"get_optimization_disconnected_cooling_capacity\"][\"schema\"] = get_csv_schema(\n        locator.get_optimization_disconnected_cooling_capacity(1, 1))\n    \n# load get_optimization_connected_cooling_capacity schema\nif not schemas[\"get_optimization_connected_cooling_capacity\"][\"schema\"]:\n    schemas[\"get_optimization_connected_cooling_capacity\"][\"schema\"] = get_csv_schema(\n        locator.get_optimization_disconnected_cooling_capacity(1, 1))"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": "# each locator method needs a \"schema\" entry (this should not output anything)\nfor lm in locators:\n    if not \"schema\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": "# the \"schema\" entry should not be `None` (this should not output anything)\nfor lm in locators:\n    if not schemas[lm][\"schema\"]:\n        print lm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "if any of the above produce printed output, update schemas.yml and re-run the notebook"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### make sure the \"used-by\" and \"created-by\" lists don't contain duplicates"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    if not \"used_by\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    if not \"created_by\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "each locator should have a \"used_by\" and a \"created_by\" - let's assume they're all lists"
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    schemas[lm][\"used_by\"] = sorted(set(schemas[lm][\"used_by\"]))\n    schemas[lm][\"created_by\"] = sorted(set(schemas[lm][\"created_by\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saving to: c:\\users\\darthoma\\documents\\github\\cityenergyanalyst\\cea\\schemas.yml\n"
    }
   ],
   "source": "# save it back\nschemas_yml = os.path.join(os.path.dirname(cea.scripts.__file__), 'schemas.yml')\nprint \"saving to:\", schemas_yml\nwith open(schemas_yml, 'w') as fp:\n    yaml.dump(schemas, fp)\nschemas = cea.scripts.schemas()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### find all schema entries that are not in glossary.csv"
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "# first: what are the missing locator methods?\nglossary_lms = set(glossary_df.LOCATOR_METHOD.values)\nschemas_lms = set(schemas.keys())\nmissing_lms = sorted(schemas_lms - glossary_lms)\nprint '\\n'.join(missing_lms)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "for each of those missing locator methods in glossary.csv, we need to append entries for each of the fields of that file. some of those files are special (the optimization checkpoints comes to mind). each glossary.csv entry has the following fields:\n\n- SCRIPT (use first \"created_by\" or \"-\", if input file)\n- LOCATOR_METHOD\n- FILE_NAME (get from schemas.yml file_path)\n- VARIABLE (this is the field name)\n- DESCRIPTION (use \"TODO\")\n- UNIT (use \"TODO\")\n- VALUES (use \"TODO\")\n- TYPE (use the first from schemas.types_found)\n- COLOR (use \"black\") - I'm not really sure we need this at all in glossary.csv?"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "for lm in missing_lms:\n    script = schemas[lm][\"created_by\"][0] if len(schemas[lm][\"created_by\"]) else \"-\"\n    file_name = schemas[lm][\"file_path\"]\n    for variable in schemas[lm][\"schema\"].keys():\n        if \"types_found\" in schemas[lm][\"schema\"][variable]:\n            type = schemas[lm][\"schema\"][variable][\"types_found\"][0] if schemas[lm][\"schema\"][variable][\"types_found\"] else \"TODO\"\n        else:\n            type = \"TODO\"\n        glossary_df = glossary_df.append({\"key\": \"{lm}!!!{variable}\".format(**locals()),\n                                          \"SCRIPT\": script,\n                                          \"LOCATOR_METHOD\": lm,\n                                          \"FILE_NAME\": file_name,\n                                          \"VARIABLE\": variable,\n                                          \"DESCRIPTION\": \"TODO\",\n                                          \"UNIT\": \"TODO\",\n                                          \"VALUES\": \"TODO\",\n                                          \"TYPE\": type,\n                                          \"COLOR\": \"black\"}, ignore_index=True)\nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### find all glossary entries that are not in schemas.yml"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "# find all locator methods not present in schemas.yml\ninvalid_lms = []  # stuff left over from previous versions of cea\nfor _, row in glossary_df.iterrows():\n    lm = row[\"LOCATOR_METHOD\"]\n    if lm not in schemas:\n        invalid_lms.append(lm)\n\nfor lm in invalid_lms:\n    glossary_df = glossary_df[glossary_df[\"LOCATOR_METHOD\"] != lm]\n\nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": "# find all variables not present in schemas.yml\n# NOTE: treat xls & xlsx files differently\ninvlaid_vars = [] # list of rows that are not valid anymore\nfor _, row in glossary_df.iterrows():\n    # we know the locator method is in here from the previous cell ;)\n    file_type = schemas[lm][\"file_type\"]\n    if file_type in {\"xls\", \"xlsx\"}:\n        assert \":\" in row[\"FILE_NAME\"], \"expected worksheet: {row}\".format(row=row)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### check to make sure all variables in schemas are present in glossary.csv"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### make sure glossary.csv (locator_method, variable) is unique"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### clean the sample_data (make longs into ints) "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
