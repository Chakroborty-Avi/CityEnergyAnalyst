{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Update glossary.csv file using data from schemas.yml\n\nThis script assumes you have a \"new\" schema obtained from running the `cea trace-inputlocator` script."
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": "import cea.scripts\nimport cea.inputlocator\nimport cea.config\nimport cea.glossary\nfrom cea.tests.trace_inputlocator import get_csv_schema\nimport os\nimport yaml\nimport json\nfrom itertools import repeat"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "schemas = cea.scripts.schemas()\nglossary_df = cea.glossary.read_glossary_df()\nlocators = schemas.keys()\n\ndef save_glossary(glossary_df):\n    glossary_df = glossary_df.sort_values([\"SCRIPT\", \"LOCATOR_METHOD\", \"FILE_NAME\", \"VARIABLE\"])\n    glossary_df.to_csv(os.path.join(os.path.dirname(cea.glossary.__file__), 'glossary.csv'),\n                  columns=[\"SCRIPT\", \"LOCATOR_METHOD\", \"FILE_NAME\", \"VARIABLE\", \"DESCRIPTION\", \"UNIT\", \"VALUES\", \"TYPE\", \"COLOR\"],\n                  index=False)\n    print(\"saved new glossary.csv - reloading\")\n    glossary_df = cea.glossary.read_glossary_df()\n    return glossary_df\nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### start by finding all entries in schemas.yml without a schema"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "the following three locator methods need \"special\" treatment:\n- get_optimization_checkpoint\n  - \"special\" schema\n- get_optimization_disconnected_cooling_capacity\n  - only present in projects with cooling network\n- get_optimization_connected_cooling_capacity\n  - only present in projects with cooling network\n  \nthis code assumes you have a \"reference-case-cooling/baseline\" in your projectroot and have run the optimization on that (e.g. run `cea workflow --workflow district-cooling-system`)"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": "config = cea.config.Configuration()\nconfig.scenario = os.path.join(config.project, \"..\", \"reference-case-cooling\", \"baseline\")\nlocator = cea.inputlocator.InputLocator(scenario=config.scenario)\n\n# load get_optimization_checkpoint schema\nif not schemas[\"get_optimization_checkpoint\"][\"schema\"]:\n    with open(locator.get_optimization_checkpoint(1), 'r') as fp:\n        get_optimization_checkpoint = json.load(fp)\n    schemas[\"get_optimization_checkpoint\"][\"schema\"] = {\n        str(key): {\"sample_data\": get_optimization_checkpoint[key],\n                   \"types_found\": None}\n        for key in get_optimization_checkpoint.keys()\n    }\n\n# load get_optimization_disconnected_cooling_capacity schema\nif not schemas[\"get_optimization_disconnected_cooling_capacity\"][\"schema\"]:\n    schemas[\"get_optimization_disconnected_cooling_capacity\"][\"schema\"] = get_csv_schema(\n        locator.get_optimization_disconnected_cooling_capacity(1, 1))\n    \n# load get_optimization_connected_cooling_capacity schema\nif not schemas[\"get_optimization_connected_cooling_capacity\"][\"schema\"]:\n    schemas[\"get_optimization_connected_cooling_capacity\"][\"schema\"] = get_csv_schema(\n        locator.get_optimization_disconnected_cooling_capacity(1, 1))"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": "# each locator method needs a \"schema\" entry (this should not output anything)\nfor lm in locators:\n    if not \"schema\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": "# the \"schema\" entry should not be `None` (this should not output anything)\nfor lm in locators:\n    if not schemas[lm][\"schema\"]:\n        print lm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "if any of the above produce printed output, update schemas.yml and re-run the notebook"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### make sure the \"used-by\" and \"created-by\" lists don't contain duplicates"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    if not \"used_by\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    if not \"created_by\" in schemas[lm]:\n        print lm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "each locator should have a \"used_by\" and a \"created_by\" - let's assume they're all lists"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": "for lm in locators:\n    schemas[lm][\"used_by\"] = sorted(set(schemas[lm][\"used_by\"]))\n    schemas[lm][\"created_by\"] = sorted(set(schemas[lm][\"created_by\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saving to: c:\\users\\darthoma\\documents\\github\\cityenergyanalyst\\cea\\schemas.yml\n"
    }
   ],
   "source": "# save it back\nschemas_yml = os.path.join(os.path.dirname(cea.scripts.__file__), 'schemas.yml')\nprint \"saving to:\", schemas_yml\nwith open(schemas_yml, 'w') as fp:\n    yaml.dump(schemas, fp)\nschemas = cea.scripts.schemas()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### find all schema entries that are not in glossary.csv"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "# first: what are the missing locator methods?\nglossary_lms = set(glossary_df.LOCATOR_METHOD.values)\nschemas_lms = set(schemas.keys())\nmissing_lms = sorted(schemas_lms - glossary_lms)\nprint '\\n'.join(missing_lms)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "for each of those missing locator methods in glossary.csv, we need to append entries for each of the fields of that file. some of those files are special (the optimization checkpoints comes to mind). each glossary.csv entry has the following fields:\n\n- SCRIPT (use first \"created_by\" or \"-\", if input file)\n- LOCATOR_METHOD\n- FILE_NAME (get from schemas.yml file_path)\n- VARIABLE (this is the field name)\n- DESCRIPTION (use \"TODO\")\n- UNIT (use \"TODO\")\n- VALUES (use \"TODO\")\n- TYPE (use the first from schemas.types_found)\n- COLOR (use \"black\") - I'm not really sure we need this at all in glossary.csv?"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "def extract_glossary_row(script, lm, file_name, variable, variable_entry):\n    if \"types_found\" in variable_entry:\n        var_type = variable_entry[\"types_found\"][0] if variable_entry[\"types_found\"] else \"TODO\"\n    else:\n        var_type = \"TODO\"\n    return {\n        \"key\": \"{lm}!!!{variable}\".format(**locals()),\n        \"SCRIPT\": script,\n        \"LOCATOR_METHOD\": lm,\n        \"FILE_NAME\": file_name,\n        \"VARIABLE\": variable,\n        \"DESCRIPTION\": \"TODO\",\n        \"UNIT\": \"TODO\",\n        \"VALUES\": \"TODO\",\n        \"TYPE\": var_type,\n        \"COLOR\": \"black\"}\n\nfor lm in missing_lms:\n    print(\"processing lm:\", lm)\n    script = schemas[lm][\"created_by\"][0] if len(schemas[lm][\"created_by\"]) else \"-\"\n    file_name = schemas[lm][\"file_path\"]\n    file_type = schemas[lm][\"file_type\"]\n    if file_type in {\"xls\", \"xlsx\"}:\n        for worksheet in schemas[lm][\"schema\"].keys():\n            for variable in schemas[lm][\"schema\"][worksheet].keys():\n                variable_entry = schemas[lm][\"schema\"][worksheet][variable]\n                ws_file_name = \"{file_name}:{worksheet}\".format(**locals())\n                row = extract_glossary_row(script, lm, ws_file_name, variable, variable_entry)\n                glossary_df = glossary_df.append(row, ignore_index=True)\n    else:\n        for variable in schemas[lm][\"schema\"].keys():\n            row = extract_glossary_row(script, lm, file_name, variable, schemas[lm][\"schema\"][variable])\n            glossary_df = glossary_df.append(row, ignore_index=True)\n        \nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### find all glossary entries that are not in schemas.yml"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": "# find all filenames for excel files that don't fit the convention (file_name, \":\", worksheet)\n# (clean glossary_df until this doesn't output anything)\nfor _, row in glossary_df.iterrows():\n    # we know the locator method is in here from the previous cell ;)\n    lm = row[\"LOCATOR_METHOD\"]\n    file_type = schemas[lm][\"file_type\"]\n    if file_type in {\"xls\", \"xlsx\"}:\n        if not \":\" in row[\"FILE_NAME\"]:\n            print \"BAD FILE_NAME:\", row[\"FILE_NAME\"]"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "# find all locator methods not present in schemas.yml\ninvalid_lms = []  # stuff left over from previous versions of cea\nfor _, row in glossary_df.iterrows():\n    lm = row[\"LOCATOR_METHOD\"]\n    if lm not in schemas:\n        invalid_lms.append(lm)\n\nfor lm in invalid_lms:\n    print \"invalid:\", lm\n    glossary_df = glossary_df[glossary_df[\"LOCATOR_METHOD\"] != lm]\n\nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "invalid VARIABLE: SC_results/SC_roofs_top_Q_kWh\ninvalid VARIABLE: SC_results/SC_roofs_top_m2\ninvalid VARIABLE: SC_results/SC_walls_east_Q_kWh\ninvalid VARIABLE: SC_results/SC_walls_east_m2\ninvalid VARIABLE: SC_results/SC_walls_north_Q_kWh\ninvalid VARIABLE: SC_results/SC_walls_north_m2\ninvalid VARIABLE: SC_results/SC_walls_south_Q_kWh\ninvalid VARIABLE: SC_results/SC_walls_south_m2\ninvalid VARIABLE: SC_results/SC_walls_west_Q_kWh\ninvalid VARIABLE: SC_results/SC_walls_west_m2\ninvalid VARIABLE: SC_total_buildings/SC_roofs_top_Q_kWh\ninvalid VARIABLE: SC_total_buildings/SC_roofs_top_m2\ninvalid VARIABLE: SC_total_buildings/SC_walls_east_Q_kWh\ninvalid VARIABLE: SC_total_buildings/SC_walls_east_m2\ninvalid VARIABLE: SC_total_buildings/SC_walls_north_Q_kWh\ninvalid VARIABLE: SC_total_buildings/SC_walls_north_m2\ninvalid VARIABLE: SC_total_buildings/SC_walls_south_Q_kWh\ninvalid VARIABLE: SC_total_buildings/SC_walls_south_m2\ninvalid VARIABLE: SC_total_buildings/SC_walls_west_Q_kWh\ninvalid VARIABLE: SC_total_buildings/SC_walls_west_m2\ninvalid VARIABLE: SC_totals/SC_roofs_top_Q_kWh\ninvalid VARIABLE: SC_totals/SC_roofs_top_m2\ninvalid VARIABLE: SC_totals/SC_walls_east_Q_kWh\ninvalid VARIABLE: SC_totals/SC_walls_east_m2\ninvalid VARIABLE: SC_totals/SC_walls_north_Q_kWh\ninvalid VARIABLE: SC_totals/SC_walls_north_m2\ninvalid VARIABLE: SC_totals/SC_walls_south_Q_kWh\ninvalid VARIABLE: SC_totals/SC_walls_south_m2\ninvalid VARIABLE: SC_totals/SC_walls_west_Q_kWh\ninvalid VARIABLE: SC_totals/SC_walls_west_m2\ninvalid VARIABLE: get_thermal_demand_csv_file/B001\ninvalid VARIABLE: get_thermal_demand_csv_file/Unnamed: 0\ninvalid VARIABLE: get_weather/EPW file variables\nsaved new glossary.csv - reloading\n"
    }
   ],
   "source": "# find all variables not present in schemas.yml\n# NOTE: treat xls & xlsx files differently\nglossary_df = cea.glossary.read_glossary_df()\ninvlaid_vars = [] # list of rows that are not valid anymore\nfor _, row in glossary_df.iterrows():\n    # we know the locator method is in here from the previous cell ;)\n    lm = row[\"LOCATOR_METHOD\"]\n    var = row[\"VARIABLE\"]\n    file_type = schemas[lm][\"file_type\"]\n    if file_type in {\"xls\", \"xlsx\"}:\n        worksheet = row[\"FILE_NAME\"].split(\":\")[1]\n        if not var in schemas[lm][\"schema\"][worksheet]:\n            print \"invalid VARIABLE: {lm}/{worksheet}/{var}\".format(**locals())\n    else:\n        if not var in schemas[lm][\"schema\"]:\n            print \"invalid VARIABLE: {lm}/{var}\".format(**locals())\n            \nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**NOTE:** for now,there are still some invalid variables in `SC_results` and `SC_total_buildings` that we'll just gloss over (pun intended). "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### check to make sure all variables in schemas.yml are present in glossary.csv"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "saved new glossary.csv - reloading\n"
    }
   ],
   "source": "glossary_df = cea.glossary.read_glossary_df().drop_duplicates(subset=[\"SCRIPT\", \"LOCATOR_METHOD\", \"FILE_NAME\", \"VARIABLE\"])\n\n# TODO: come up with a solution for these...\nEXCLUDE_LOCATOR_METHODS = {\"SC_totals\", \"SC_results\", \"SC_total_buildings\"}\n\nfor lm in sorted(schemas.keys()):\n    if lm in EXCLUDE_LOCATOR_METHODS:\n        continue\n    schema = schemas[lm][\"schema\"]\n    file_type = schemas[lm][\"file_type\"]\n    file_name = schemas[lm][\"file_path\"]\n    script = schemas[lm][\"created_by\"][0] if schemas[lm][\"created_by\"] else \"-\"\n    if file_type in {\"xls\", \"xlsx\"}:\n        for worksheet, schema in schemas[lm][\"schema\"].items():\n            for var in schema.keys():\n                matches = glossary_df[(glossary_df[\"LOCATOR_METHOD\"] == lm)\n                                       & (glossary_df[\"VARIABLE\"] == var)\n                                       & (glossary_df[\"FILE_NAME\"] == file_name + \":\" + worksheet)].values\n                if len(matches) != 1:\n                    glossary_df = glossary_df.append({\n                        \"SCRIPT\": script,\n                        \"LOCATOR_METHOD\": lm,\n                        \"FILE_NAME\": \"{file_name}:{worksheet}\".format(**locals()),\n                        \"VARIABLE\": var,\n                        \"DESCRIPTION\": \"TODO\",\n                        \"UNIT\": \"TODO\",\n                        \"VALUES\": \"TODO\",\n                        \"TYPE\": \"TYPE\",\n                        \"COLOR\": \"black\",\n                    }, ignore_index=True)\n                    print(\"ADDED: {lm}/{worksheet}/{var} ({matches})\".format(**locals()))\n    else:\n        for var in schema.keys():\n            # check if it exists in `glossary.csv`\n            matches = glossary_df[(glossary_df[\"LOCATOR_METHOD\"] == lm) & (glossary_df[\"VARIABLE\"] == var)][\"VARIABLE\"].values\n            if len(matches) != 1:\n                glossary_df = glossary_df.append({\n                    \"SCRIPT\": script,\n                    \"LOCATOR_METHOD\": lm,\n                    \"FILE_NAME\": file_name,\n                    \"VARIABLE\": var,\n                    \"DESCRIPTION\": \"TODO\",\n                    \"UNIT\": \"TODO\",\n                    \"VALUES\": \"TODO\",\n                    \"TYPE\": \"TYPE\",\n                    \"COLOR\": \"black\",\n                }, ignore_index=True)\n                print(\"ADDED: {lm}/{var} ({matches})\".format(**locals()))\n                \nglossary_df = save_glossary(glossary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### make sure glossary.csv (locator_method, variable) is unique"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### clean the sample_data (make longs into ints) "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
