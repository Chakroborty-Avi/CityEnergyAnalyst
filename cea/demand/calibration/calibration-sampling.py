from SALib.sample import latin
import pandas as pd
from scipy.stats.distributions import triang


def latin_sampler(locator, num_samples, variables, variable_groups = ('ENVELOPE', 'INDOOR_COMFORT', 'INTERNAL_LOADS')):


    # get probability density function of variables of interest
    dataframe_uncertainty_database = pd.concat([pd.read_excel(locator.get_uncertainty_db(), group, axis=1) for group in variable_groups])
    pdf = dataframe_uncertainty_database[['name' == variables]]

    # define the problem
    problem = {'num_vars': pdf.name.count(), 'names': pdf.name.values, 'bounds': bounds, 'groups': None,
               'N': num_samples, 'method': method}
    problem.update(sampler_parameters)



def LHC(problem, number_samples, output_path):
    """Generate model inputs using Latin hypercube sampling (LHS).

    Returns a NumPy matrix containing the model inputs generated by Latin
    hypercube sampling.  The resulting matrix contains N rows and D columns,
    where D is the number of parameters.

    Parameters
    ----------
    problem : dict
        The problem definition
    N : int
        The number of samples to generate
    calc_second_order : bool
        Calculate second-order sensitivities (default True)
    """
    result = latin(problem, number_samples)


    result.
    return result